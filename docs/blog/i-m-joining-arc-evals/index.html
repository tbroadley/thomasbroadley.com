<!DOCTYPE html>
<html lang="en">
  <head>
    <title>I&#39;m joining ARC Evals—Thomas Broadley</title>

    <script type="application/ld+json">
      {
        "@context": "https://schema.org",
        "@type": "SocialMediaPosting",
        "headline": "I&#39;m joining ARC Evals",
        "datePublished": "2023-11-03",

        "dateModified": "2023-11-05",

        "author": [
          {
            "@type": "Person",
            "name": "Thomas Broadley",
            "url": "https://thomasbroadley.com"
          }
        ]
      }
    </script>

    <meta
      content="I&#39;m joining ARC Evals—Thomas Broadley"
      property="og:title"
    />
    <meta
      content="I&#39;m joining ARC Evals—Thomas Broadley"
      property="twitter:title"
    />
    <meta content="en_US" property="og:locale" />
    <meta
      content="Why I think this role at this organization will let me meaningfully reduce AI x-risk."
      name="description"
    />
    <meta
      content="Why I think this role at this organization will let me meaningfully reduce AI x-risk."
      property="og:description"
    />
    <link
      href="https://thomasbroadley.com/blog/i-m-joining-arc-evals/"
      rel="canonical"
    />
    <meta
      content="https://thomasbroadley.com/blog/i-m-joining-arc-evals/"
      property="og:url"
    />
    <meta content="Thomas Broadley" property="og:site_name" />
    <meta content="article" property="og:type" />
    <meta content="summary" name="twitter:card" />

    <meta charset="utf-8" />

    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
    <link rel="manifest" href="/manifest.json" />
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5" />
    <meta name="theme-color" content="#ffffff" />

    <link
      rel="alternate"
      type="application/rss+xml"
      href="https://thomasbroadley.com/blog/rss.xml"
    />

    <script>
      (function (i, s, o, g, r, a, m) {
        i["GoogleAnalyticsObject"] = r;
        (i[r] =
          i[r] ||
          function () {
            (i[r].q = i[r].q || []).push(arguments);
          }),
          (i[r].l = 1 * new Date());
        (a = s.createElement(o)), (m = s.getElementsByTagName(o)[0]);
        a.async = 1;
        a.src = g;
        m.parentNode.insertBefore(a, m);
      })(
        window,
        document,
        "script",
        "https://www.google-analytics.com/analytics.js",
        "ga"
      );

      ga("create", "UA-96230441-1", "auto");
      ga("send", "pageview");
    </script>

    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <link
      href="https://fonts.googleapis.com/css?family=Roboto+Slab|Roboto|Source+Code+Pro"
      rel="stylesheet"
    />
    <link href="/index.css" rel="stylesheet" />

    <style>
      p.image > img {
        max-width: 100%;
      }

      @media (max-width: 599px) {
        p.image {
          overflow-x: auto;
        }

        p.image > img {
          max-width: 200%;
        }
      }

      p.image-centered {
        text-align: center;
      }

      p.blogchain {
        display: flex;
        justify-content: center;
      }

      p.blogchain > * {
        display: block;
        text-align: center;
      }

      p.blogchain > *:not(:last-child) {
        margin-right: 1rem;
      }

      #header-image-figure {
        width: 50%;
        float: right;
        margin-top: 0;
        margin-right: 0;
      }

      #header-image-figure > img {
        width: 100%;
      }

      #header-image-figure > figcaption {
        margin-top: 0.5rem;
        font-size: 0.75rem;
        line-height: 1rem;
        font-style: italic;
      }

      @media (max-width: 399px) {
        #header-image-figure {
          width: 100%;
          float: none;
          margin-left: 0;
          margin-right: 0;
        }
      }
    </style>

    <link
      href="https://unpkg.com/prismjs@1.21.0/themes/prism.css"
      rel="stylesheet"
    />
  </head>
  <body>
    <nav>
      <a href="..">All posts</a>
      &middot;
      <a href="../rss.xml">RSS</a>
    </nav>

    <article>
      <header>
        <h1>
          I&#39;m joining ARC Evals
        </h1>
        <p class="timestamp">
          <span data-pagefind-meta="created">2023-11-03</span>
          (last modified
          <span data-pagefind-meta="last modified">2023-11-05</span>)
        </p>

        <p class="blogchain">
          <a
            href="../reproducing-arc-evals-recent-report-on-language-model-agents"
            >&larr;</a
          >

          <a href="../tags/ai-x-risk" data-pagefind-filter="tag">
            AI x-risk
          </a>

          <a href="../dangerous-capability-evaluations-for-ai">&rarr;</a>
        </p>

        <hr />
      </header>

      <section>
        <p>
          A little over two months ago, I
          <a href="/blog/i-m-leaving-my-job-next-ai-x-risk/"
            >left my job at Faire to figure out how I could help reduce
            existential risk from AI</a
          >. Today, I'm happy to announce that I've accepted a position as a
          Member of Technical Staff at
          <a href="https://evals.alignment.org/">ARC Evals</a>, a project of the
          <a href="https://evals.alignment.org/">Alignment Research Center</a>.
          According to its website, the project's goal is to "assess[] whether
          cutting-edge AI systems could pose catastrophic risks to
          civilization".
        </p>
        <p>
          I first applied to ARC Evals in April of this year. Unfortunately, I
          wasn't offered a position at the time. However, after I
          <a
            href="/blog/reproducing-arc-evals-recent-report-on-language-model-agents/"
            >replicated some of ARC Evals' recent work</a
          >, my profile came back to the team's attention. The last step of my
          interview process was a two-week in-person work trial in Berkeley,
          California. (Getting to Berkeley was a small adventure all by itself.
          In a single day, I flew from Los Angeles to Vancouver, drove to a
          library to print documents, drove back to the Vancouver airport, went
          through US customs to get TN status, and flew on to San Francisco to
          begin my work trial.)
        </p>
        <p>
          ARC Evals' current focus is checking whether AI models can
          <em>autonomously replicate</em> given the right tools. We explore
          questions like, "If we give GPT-4 access to a web browser, could it
          use that to conduct a phishing campaign to gain money or influence? If
          we create a cloud server for it, can it set up more servers and copy
          itself to them? Can we fine-tune these models to be better at these
          tasks?" ARC Evals is also "exploring the idea of developing safety
          standards that AI companies might voluntarily adhere to, and
          potentially be certified for".
        </p>
        <p>
          I'm hopeful about this work because many smart, sensible people
          disagree about how likely AI is to cause a catastrophe. Evaluations
          like those that ARC Evals develops could convince skeptics that future
          AI models pose a serious risk of catastrophe. Also, if AI labs develop
          or deploy models powerful enough to pose such a risk, we want to
          detect that as soon as possible. Of course, if ARC Evals and other
          evaluators can't develop convincing demonstrations of catastrophic
          risk, it could be evidence that AI doesn't pose as much danger as we
          thought. I'd welcome that too!
        </p>
        <p>
          More specifically, I see two ways ARC Evals' work could meaningfully
          reduce AI x-risk. First, maybe ARC Evals's demonstrations of
          catastrophic risk in controlled environments can convince policymakers
          to pause AI progress while humanity mitigates this risk. Second, the
          kinds of evaluations that ARC Evals develops might form an important
          part of governmental regulation of AI. For example, I'd be very happy
          to live in a world where governments enforce that every AI lab adopt
          and follow something like a stronger version of
          <a
            href="https://www.anthropic.com/index/anthropics-responsible-scaling-policy"
            >Anthropic's Responsible Scaling Policy</a
          >.
        </p>
        <p>
          So far at ARC Evals, I've focused on improving our internal software
          used to develop new evaluations and test AI models on them. My web
          development skills are a great fit for this work. Plus, I have a soft
          spot for internal tools: I love helping my friends and coworkers be
          more productive.
        </p>
        <p>
          I'm really excited! Until a couple of weeks ago, I was seriously
          worried that I might not find a way to meaningfully reduce short-term
          AI x-risk. There aren't that many AI safety organizations or jobs and
          conducting independent research seemed daunting. Like I said in a
          previous post, "I suspect my current work habits and intrinsic
          motivation aren't up to the task of figuring this out". I'm so glad
          I've found a concrete way to reduce AI x-risk and a team of kind,
          motivated people to work with on it. Not to say that working at ARC
          Evals will be a cakewalk. I plan to seriously up my productivity game
          and evaluate continuously whether I'm
          <a href="https://www.lesswrong.com/tag/twelfth-virtue-the"
            >cutting the enemy</a
          >: whether my work is actually reducing AI x-risk.
        </p>
        <p>I'll let you know how it goes!</p>
      </section>
    </article>

    <script src="https://unpkg.com/prismjs@1.21.0/components/prism-core.min.js"></script>
    <script src="https://unpkg.com/prismjs@1.21.0/plugins/autoloader/prism-autoloader.min.js"></script>
  </body>
</html>
